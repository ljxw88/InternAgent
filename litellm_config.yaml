# LiteLLM Proxy Configuration for InternAgent + GitHub Models
# Primary model: Claude Sonnet 4.6 (claude-sonnet-4-5) via GitHub Models
#
# Usage:
#   litellm --config litellm_config.yaml --port 4000
#
# Prerequisites:
#   - GitHub account with access to GitHub Models (free tier available)
#   - GITHUB_TOKEN set in your environment (or .env file)
#     Get token at: https://github.com/settings/tokens
#   - litellm installed: pip install litellm

model_list:
  # Primary model: Claude Sonnet 4.6 via GitHub Models
    # GitHub Models registry ID: claude-sonnet-4-5
      # LiteLLM prefix for GitHub Models: "github/"
        - model_name: claude-sonnet-4-5         # Alias InternAgent will call
            litellm_params:
                  model: github/claude-sonnet-4-5     # GitHub Models registry ID
                        api_key: "os.environ/GITHUB_TOKEN"  # reads GITHUB_TOKEN from environment

                          # Fallback: claude-sonnet-4-5 (earlier snapshot, same model family)
                            - model_name: claude-sonnet-4-5-fallback
                                litellm_params:
                                      model: github/claude-sonnet-4-5
                                            api_key: "os.environ/GITHUB_TOKEN"

                                              # Optional alternative: GPT-4o (uncomment if needed)
                                                # - model_name: gpt-4o
                                                  #   litellm_params:
                                                    #     model: github/gpt-4o
                                                      #     api_key: "os.environ/GITHUB_TOKEN"

                                                      litellm_settings:
                                                        # Drop unsupported OpenAI-specific parameters (important for Claude compatibility)
                                                          drop_params: true
                                                            # Request timeout in seconds
                                                              request_timeout: 120

                                                              router_settings:
                                                                # Retry on transient failures
                                                                  num_retries: 3

                                                                  general_settings:
                                                                    # master_key must match OPENAI_API_KEY in your .env
                                                                      master_key: "litellm-proxy"
                                                                        port: 4000
